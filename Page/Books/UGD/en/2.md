# _Ⅱ -_ Analysing Geochemical Data {docsify-ignore}

**_Using Geochemical Data_**

## Introduction

The last 50 years have seen the growth of a large body of literature on the statistical treatment of geochemical data. Some of this literature is in the form of warnings to the geochemical community that their practices are not sufficiently statistically rigorous. Other articles are concerned with improving the statistical techniques current amongst geochemists and provide methods which are more appropriate to some of the peculiar properties of geochemical data. Further aspects of geo-statistics are very specialised and apply particularly to the fields of ore reserve estimation and exploration geology (see, for example, Clark and Harper, 2007). These are not considered here.

However, many statisticians are not practising geochemists and, similarly, many geochemists are unfamiliar with the statistical literature. This may in part be because this literature is specific to mathematical geology and is written for other mathematicians rather than for geochemists. The net effect is that geo-statisticians have been advising geochemists in the practice of their art for many years and yet rarely have the two communities come together to fully address their common concerns.

The purpose of this chapter, therefore, is to draw the attention of geochemists to some of the issues which our statistician colleagues have raised and to evaluate these in the context of presenting and interpreting geochemical data. This is not intended as a review of statistics as applied to geochemistry. There are excellent books for that purpose; see, for example, Waltham (2000), Davis (2002) and Reimann et al. (2008), and as well as the web-based reference NIST/SEMATECH e-Handbook of Statistical Methods. Rather, this chapter presents a discussion of some areas of statistics which directly relate to geochemistry. We assume an introductory level of statistical knowledge. See Box 2.1 for brief definitions of the statistical terms used.

Many of the statistical calculations described in this text can be carried out in an Excel spreadsheet using the data analysis ‘Add-In’ and the Real Statistics Using Excel ‘Add-In’ (available at www.real-statistics.com/). There are also commercial statistical packages that run in Excel and shareware packages that run on other platforms such as the java-based Stacato. Alternatively, more sophisticated analyses can be carried out in bespoke statistical packages such as SPSS (statistical package for the social sciences, officially IBM SPSS Statistics); there are also applications in STATISTICA (by Dell), Python, Matlab and R (see Van den Boogaart and Tolosana-Delgado, 2013; Janousek et al., 2016).

The central problem which geochemists have to address when considering the analysis of geochemical data is the unusual nature of geochemical data. Composition is expressed as a part of a whole, either as a percentage or parts per million. This type of data is ‘constrained’ or ‘closed’ and is not amenable to standard statistical analysis, which is generally designed for unconstrained data. The particular nature of geochemical data raises all sorts of uncomfortable questions about the application of standard statistical methods in geochemistry. These issues are addressed in Sections 2.2 and 2.7. Geochemical data also has ‘outliers’ which geochemists typically exclude but which statistics can now address, for example, Pernet et al. (2013). In addition, geochemical data are often derived from samples collected from limited outcrop in specific locations over a limited amount of time and yet the statistical ‘validity’ of the sampling process is seldom considered.

With these caveats in mind, let us turn to the application of selected statistical techniques which are widely used in geochemistry. Typically, geochemical data are multivariate; that is, many measurements are made on each sample (e.g., wt.% oxide data, trace element data and isotopic data are all measured on the same sample). For this reason it is necessary to use multivariate statistical methods such as those discussed in Section 2.8; these include correlation matrices, covariance matrices, principal component and factor analysis, and discriminant analysis. Sometimes, however, it is necessary to use multiple bivariate comparisons where only two of the many variables are compared at any one time, as for example in the use of correlation coefficients (Section 2.4) or regression analysis (Section 2.5). Univariate statistics such as the mean, median, mode and standard deviation, as well as probability functions (Section 2.3), are also needed to describe the distribution of the data.

## A Statistical Approach?

### Geochemical Investigation versus Statistical Trials

It should be acknowledged at the outset that a geochemical investigation may be constrained by funding, location, time available at the study area, the number and types of samples that are collected, the limited spatial distribution of those samples and the type of geochemical analyses that can be performed, so that structuring a study as a proper ‘statistical trial’ is unlikely. Geochemists do not work from carefully controlled trials, but from their often limited observations. Consequently, from inception, geochemical investigations may be regarded as somewhat statistically handicapped because the project may be constrained by small sample numbers and limited sample diversity. This contributes to the statistical limitations associated with geochemical data described in Section 2.2.2.

### Statistical Limitations Associated with Geochemical Data

There are several important issues associated with geochemical data that complicate their statistical analysis. To varying degrees these apply to all geochemical studies and include the following:

- Constrained data

- Data with variable (non-uniform) errors

- Small datasets

- Outliers.

#### Constrained or Closed Data and the Constant Sum Problem

Data expressed as part of a whole (percentages or parts per million) are known as compositional data. Geochemists are used to expressing the major element compositions of rocks and minerals as percentages, so that the sum of the major elements will always be about 100% (a constant sum). This has to be in order to compare one chemical analysis with another. This standard form, which is universally used by geochemists for both rock and mineral analyses, is a source of concern to statisticians who for 60 years have been informing geochemists that they are working in a minefield of spurious correlations and that their interpretation of major and trace element chemistry is potentially unsound.

In brief, the problem with compositional data is as follows. Percentages are complex ratios containing variables in their denominators which represent all the constituents being examined. Thus, components of percentage data are not free to vary independently. As the proportion of one component increases, the proportion of one or more other components must decrease. These properties were more formally summarised by Pawlowsky-Glahn and Egozcue (2006), who showed that compositional data:

- convey only relative information

- are always positive, thus

- are not free to range between +/− infinity (in statistical terms they are constrained rather than unconstrained)

- are parts of a composition that sum to a constant (in statistical terms they form a closed dataset)

- are therefore not amenable to standard statistical tests which are generally devised for open datasets with unconstrained variables in the range +/− infinity.

The consequences of these properties for compositional data in geochemistry are the following:

- The summing of analyses to 100% forces a correlation between components of the dataset because the loss or gain of any component causes a change in the concentration of all the other components. The problem is illustrated by Meisch (1969), who demonstrated that an apparently significant correlation between two oxides may have arisen through the closure process from an actual correlation between two other oxides. These induced correlations may or may not conceal important geological correlations. In terms of correlation theory, the usual criterion of independent variables does not hold nor is variance and covariance within the dataset independent.

- There is a negative bias in correlations. If one component has a significantly higher value than all the other components – such as SiO2 in silicate rocks – then bivariate graphs of SiO2 and the other components will show a marked negative tendency. This attribute of percentage data is fully explored by Chayes (1960) and is illustrated in Figure 3.13.

- Data subsets or sub-compositions, as frequently used in triangular diagrams such as the AFM diagram (Section 3.3.3), do not reflect the variation present in the ‘parent’ dataset. Aitchison (1986, Table 3.1) showed that the correlation coefficient between pairs of variables changes substantially in data subsets and that there is no apparent pattern to the changes. In addition, data subsets can show different rank orderings from the parent dataset. For example, in the AFM data subset (Na2O + K2O, FeOT, MgO) the variance may be A > F > M, but in the parent dataset the variance may be F > A > M.

- The means of compositional datasets have little significance. Woronow (1990) and Woronow and Love (1990) have argued that because of the closure problem the means of percentage (and ppm) data have no interpretative value. They show that an infinite number of possible changes can explain an observed change in the mean of compositional data such that it is impossible to produce a meaningful statistical test to evaluate the similarities between means of compositional data. This observation has important implications for approaches frequently used in geochemistry such as discriminant analysis (Section 2.8.2).

It should be noted that the problem of closure is not eliminated when the data are transformed by some scaling process (Whitten, 1995), nor is it removed when the problem is presented in graphical rather than statistical form. Attempts to sidestep closure by relating all samples to a given reference curve as in the ‘sliding normalisation diagrams’ of Liegeois et al. (1998) or the ‘delta diagrams’ of Moyen et al. (2009) do not avoid the fundamental issues outlined above. Furthermore, the closure problem remains even when the constituents of an analysis do not sum exactly to 100%, due to analytical error or incomplete analysis, for there is always the category of ‘others’ (i.e., the missing data) which can be used to produce a sum of 100%. A more detailed discussion of how these problems have been approached in the geo-statistical community is given in Section 2.7.

#### Non-uniform Errors (Heteroscedasticity)

The statistical analysis of geochemical data depends on the errors associated with the variables (oxides and elements). In the context of geochemical analysis, error means the combined systematic and random error (the uncertainty) that is associated with an analytical measurement. All geochemical analyses have associated errors but these are not the same for all the components of a single analysis for the magnitude of the error depends upon what is being measured and the analytical method(s) used. For example, analytical errors associated with XRF analysis vary as a function of both the element mass and the concentration of the element and therefore different elements will have different associated errors. Further, in a comparison of data from one laboratory with another the errors associated with each lab will be different. In some instances data measured in different ways are used together, giving rise to different scales of error. An example would be the measurement of major elements by XRF and trace elements by mass spectrometry. This variability in the nature of the errors associated with geochemical data means that it is necessary to select the statistical approach which is most appropriate to the data.

#### Small n

Many statistical functions are dependent on the number (n) of variables involved because they assume a normal or Gaussian distribution. This often takes the form of the Student’s t-distribution, which is used, for example, when determining statistical significance of the difference between two means, the confidence interval of the difference between two means, and in linear regression analysis. In a statistical calculation the term degrees of freedom refers to the number of variables that are free to vary; for example, in bivariate plots there are two degrees of freedom because there are two variables. As the number of degrees of freedom increases, the t-distribution approaches a normal distribution. However, a specific variable in a small sample set (n) may not be normally distributed and so violates the assumption of many statistical approaches.

#### Outliers

Many geochemical datasets contain outliers, that is, single values completely outside the range of the other measured values. Outliers can occur by chance in any distribution, but they often indicate a measurement error, the combination of unrelated sub-populations, or that the population has a spurious (non-Gaussian) distribution.

### Can We Address These Limitations?

The following steps can be taken to address the limitations that geochemists face with their data. These are explored in detail in this chapter but are summarised here:

- Constrained geochemical data, in which the standard deviation of the error is not constant with respect to the variables involved, may be explored using weighting (Section 2.5) and data transformation (Section 2.7). Both methods work well and often provide similar results.

- Choosing a statistical method appropriate for the type of data involved requires knowledge of the types of errors associated with the data and whether these are variable or constant errors.

- Many statistical tests are designed for larger datasets. At a fundamental level statistical analysis generally works best above some nominal minimum number of samples or analyses, and so increasing n will increase the statistical options.

- All datasets need to be assessed for outliers and there are various tests that help to recognize them (Sections 2.3 and 2.4); once recognized, appropriate steps can be taken.

- Robust statistical methods are those which work even when the original assumptions of the test are not strictly fulfilled, such as non-Gaussian data, small n, variable errors and the presence of outliers, and these can be applied to minimise the effect of these assumptions.

## Histograms, Averages and Probability Functions

### Histograms

The histogram is a common way to show how data are distributed relative to the number of samples and shows the density of the data distribution. A histogram quickly and easily shows whether the data distribution is Gaussian or non-normal, symmetrical or skewed, uni-, bi- or multi-modal. However, this is in part a function of the way in which the data are allocated to intervals or ‘bins’ in the histogram. As general rule bin width = √n, where n is the number of samples in the dataset. It is clear that bin widths reduce in size the larger the dataset and as n increases the histogram ‘steps’ are eventually so small that the plot appears to have a smooth, step-free pattern. This is the density function shown as the blue line in Figure 2.1a and discussed in Section 2.3.3.

![ Histograms, density functions, and probabilities.](pic/fig2-1.png "直方图、密度函数和概率")

_(a) The density function (blue line) is defined by the observations shown in the histogram; the area under the curve is equal to 100%. The blue shaded region represents 50% of the density function. The mode, median and mean are identical for symmetrical normally distributed datasets, where the mode is the most frequent number; the median is the middle number in the data list, and the mean is the average of all numbers. (b) The mode, median and mean are different for asymmetrical, non-normally distributed data. (c) Histogram, probability density curves (PDCs) and kernel density estimate (KDE) for Permian sandstones from the New Siberian Islands of Russia (from Pease et al., 2015; with permission from the Geological Society of London). The histogram of detrital zircon U-Pb ages (light blue rectangles; bin sizes = 15 Ma, maximum 2σ from dating method) and the PDCs for these data (blue line). The KDE for the same data is shown by the solid grey line and should mimic the histogram. The PDC of Permian sandstone from Taimyr (grey dotted line; data from Zhang et al., 2013) shows a strong similarity to that of the New Siberian Island data. (d) The cumulative probability function sums each of the observations in all bins up to the value of the bin specified. If the histogram has a normal distribution, the cumulative probability function will have a typical symmetrical, sigmoidal shape (blue line). On this plot the probability of obtaining a value of ≤X is 0.1 (or 10%) and the probability for obtaining a value ≤X2 is 70%. (e) Cumulative probability curves (CPCs) for samples from different Caledonian Nappes showing similarity of samples (F-1 and F-3) within the Gaisa Nappe (after Zhang et al., 2016, with permission from Elsevier). Note the two datasets were generated at the same analytical facility using the same methods and have similar analytical errors._

### Averages

Geochemists frequently use ‘average values’ in order to present their data in summary form. Also, average rock compositions are sometimes used as a reference by which to assess the composition of other rock compositions. For example, the average values of mid-ocean ridge basalts or of chondritic meteorites are frequently used as normalising values in trace element studies (see, e.g., Table 4.7). Averages are also used in analytical geochemistry where a signal is measured several times and then an average of the measurements taken.

Rock (1987, 1988) has drawn attention to the way in which geochemists calculate average values and has shown that the methods most frequently used are inadequate. The most commonly used summary statistic is the arithmetic mean, with the standard deviation sometimes quoted as a measure of the spread of the data. The geometric mean, the median, the mode, and variance are less frequently used. These terms are defined in Box 2.1.

However, when averaging geochemical data the arithmetic mean is an inappropriate choice for three reasons. First, it assumes that the sample population is either normally or log-normally distributed (Figure 2.1a, b). For many geological datasets this is not the case, nor is there any a priori reason why it should be so. Therefore, the normality of geochemical data should never be assumed. Second, many geochemical datasets contain outliers that can cause extreme distortion to an arithmetic mean if not recognised and excluded. Furthermore, as argued by Woronow (1990) and Woronow and Love (1990), the arithmetic mean of compositional data has no interpretative value. Similar problems can occur with estimates of the standard deviation and with the geometric mean. Rock (1988) emphasised that in the statistical literature, of the various methods used to assess average values, the ‘mean and standard deviation consistently performed the worst’. For this reason the median is a more robust measure of the average (Figure 2.1b) and is to be preferred. Rock (1988) also showed that the calculation of several robust estimates can identify outliers in the dataset from the inconsistencies that arise from between the calculated values.

### Probability Functions and Kernel Density Estimates

Probability functions address either continuous or discrete random variables in a dataset and provide a means of assessing the likelihood of a certain random variable assuming a certain value. The probability density function (PDF) is for continuous random variables, where the area under the probability density curve or trace indicates the interval in which a variable will fall within the context of a set of continuous random variables. The probability density function can also be expressed as a cumulative probability or cumulative distribution (see Section 2.3.4). The p function is for discrete random variables and represents the relative probability of a (random) variable having a particular value; in this case the probability is measured at a single point.

The Kernel density estimate (KDE) is an approximation of the density function and its main advantage is that it is a continuous function that does not require binning. The KDE is calculated from the data using a specified bandwidth combined with a weighting (kernel) function. The KDE is a smoothed and continuous curve that is sensitive to bandwidth and the kernel function used (Gaussian, uniform or triangular). KDEs are well suited to comparing the data distribution of multiple samples because multiple KDEs are easily plotted on a single diagram. The KDE can be calculated in Excel using the NumXL add-in and most statistical software packages. Java-based freeware for plotting KDEs is also available (Vermeesch, 2012).

Probability functions and KDEs are commonly used in the presentation of detrital zircon U-Pb age data in sediment provenance analysis. Such data may be displayed as probability density curves (PDCs) (Figure 2.1c). PDCs are similar to PDFs, but they are not equivalent (see the discussion in Vermeesch, 2018a). PDCs represent the sum of a number of Gaussian distributions (one for each datum) whose means and standard deviations correspond to the individual ages and their analytical uncertainties. Consequently, a PDC graph favours precise data (sharp peaks) over imprecise data (smooth lows) as illustrated in Figure 2.1c.

Pease et al. (2015) compared PDC patterns of Permian sandstones from the New Siberian Islands and Taimyr, Arctic Russia, and used their similarity to suggest that both samples represented part of the same foreland basin of the Uralian Orogen in Arctic Russia (Figure 2.1c). The PDC and KDE for the New Siberian Island sample are similar and mimic the histogram of the detrital zircon U-Pb age data. We recommend that PDCs or KDEs are displayed with a histogram of the data as this validates the data handling and allows any irregularities in the dataset to be identified. However, it should be noted that PDCs lack a firm theoretical basis and that large or highly precise datasets can produce counter-intuitive results (Vermeesch, 2012).

### Cumulative Distribution Function

The probability density function (PDF) can also be expressed as a cumulative probability or cumulative distribution (Figure 2.1d). This makes it possible to determine the probability of a particular value occurring within a population. The cumulative distribution function (CDF) represents the probability that a random variable will have a value less than or equal to another value and can be used as a measure of the cumulation of probability up to a given value. It is useful, for example, to quantify the percentage of samples plotting above or below a certain value. In addition, multiple datasets can be combined on a single graph to allow a visual assessment of sample similarity. This method is commonly applied in sediment provenance analysis and was used by Zhang et al. (2016) to distinguish between different Caledonian Nappe complexes (Figure 2.1e).

### Chi-Square ( $χ^2$ ) ‘Goodness of Fit’ Test

The Pearson’s chi-square test assesses the ‘goodness of fit’ between a theoretical distribution (such as normal, binomial or Poisson) and the empirical data distribution. It can be applied to any univariate dataset for which the cumulative distribution function (Section 2.3.4) can be calculated. The chi-square test is applied to data separated into intervals, or ‘binned’. For non-binned data it is necessary first to calculate a histogram or frequency table before performing the chi-square test. In its simplified form the equation for χ2 is:

$$χ^2=\frac {\sum (O−E)^2} {E}$$

where $χ^2 =$ chi-squared, O = the observed value, and E = the expected value. The $χ^2$ test assumes that the data are a random sampling of binned data and it is sensitive to sample size. If the number of points in the bin interval is too small (<5) or the total number of observations is too small (<10), spurious results can be obtained (Koehler and Larnz, 1980).

$χ^2$ tests the null hypothesis against the alternative hypothesis with respect to an associated degree of freedom at some level of significance or probability (Table 2.1). The null hypothesis (H0) typically assumes that there is no significant difference between the observed and expected values (the alternative hypothesis would then be that there is a significant difference between the observed and expected values). The probability that H0 holds true may be estimated for different levels of significance, usually at the 5% (0.05) level or the 1% (0.01) level. Alternatively, these values may be expressed as confidence limits, in this case 95% or 99%, respectively. The probability values in Table ## 1 represent the minimum values needed to reject H0. When $χ^2$ is greater than the table value, H0 is rejected; when $χ^2$ is less than the table value, H0 is accepted; small values for $χ^2$ generally lead to acceptance of H0. The Pearson chi-square test is a standard function available in Excel and most statistical packages.

## Correlation

One of the most important tasks for the geochemist using geochemical data is to determine whether or not there are associations between the oxides or elements. For example, in the list of analyses of tonalitic and trondhjemitic gneisses in Table 2.2, do the oxides CaO and Al2O3 vary together? Is there a linear relationship between $K_2O$ and $Na_2O$ ? This type of question is traditionally answered using the statistical technique of correlation.

### The Pearson Linear Correlation Coefficient (r)

Correlation may be defined as a measure of the strength of association between two variables measured on a number of individuals and is quantified using the Pearson product-moment coefficient of linear correlation, usually known as the correlation coefficient (r). Thus, the calculation of the correlation coefficient between CaO and $Al_2O_3$ or $K_2O$ and $Na_2O$ can provide an answer to the questions asked above. When, as is normal in geochemistry, only a sample of the total population is measured, the sample correlation coefficient (r) is calculated using the expression:

$$r = \frac{covariance (x, y)}{\sqrt{variance (x) \times variance (y)}}$$

_where there are n values of variable x (x1 … xn) and of variable y (y1 … yn)._

#### The Significance of the Correlation Coefficient

The sample correlation coefficient (r) is an estimate of the population correlation coefficient (ρ), the correlation that exists in the total population of which only a sample has been measured. It is important to know whether a calculated value for r represents a statistically significant relationship between x and y. That is, does the relationship observed in the sample hold for the population? The probability that this is the case is made with reference to a table of r values (Table 2.3). For a given number of degrees of freedom (the number of samples minus the number of variables, which in the case of a bivariate plot = 2), r is tabulated for different significance levels. These r values represent the minimum value needed to reject the null hypothesis (H0). In this case, the null hypothesis is that the correlation coefficient of the population is zero (ρ = 0) at the specified level of significance.

| H P         |          Description |
| :---------- | -------------------: |
| H0:ρ=0      |      null hypothesis |
| H1:ρ≠0, or  | two–sided hypothesis |
| H1:ρ>0orρ<0 | one–sided hypothesis |

Two sets of tables are provided to account for both positive and negative r values. A one-sided test is used when the alternative hypothesis (H1) is either ρ > 0 or ρ < 0 (the region of rejection lies in a single direction). The two-sided test is used when the alternative hypothesis is ρ ≠ 0 (the region of rejection occurs in two directions). For example, the dataset in Table 2.2 contains 31 samples and the calculated correlation coefficient between CaO and Al2O3 is r = 0.568 (Table 2.4a). The tabulated values for r (assume a one-sided test) shows that at the 5% significance level and 29 degrees of freedom (n − 2), the tabulated value for r = 0.301 (Table 2.3a). Since the calculated value (0.568) is greater than the tabulated value (0.301), the correlation coefficient in the sample is statistically significant at the 5% level. That is, there is 95% chance that the relationship observed in the sample also applies to the population. Hence, the null hypothesis that ρ = 0 is rejected.

#### Assumptions Associated with the Correlation Coefficient

The Pearson product-moment coefficient of linear correlation is based upon the following assumptions:

1. The units of measurement are equidistant for both variables.

2. There is a linear relationship between the variables.

3. Both variables should be normally or nearly normally distributed.

The Pearson correlation coefficient is vulnerable to data outliers of any kind, high and low, as well as deviations from the main trend of the data array. Assumption (iii) is regarded as an important prerequisite for linear correlation. However, this is not always evaluated; it is the variation of y from the estimated value of y for each value of x that must be normally distributed and rarely is the sample population large enough for this criterion to be satisfactorily tested. This means that the use of Pearson’s method requires a careful study of the univariate distribution of each of the variables before determination of r. In addition, it is useful to routinely investigate a log-transformation of the data (Section ## 7) before calculating r.

A robust linear correlation in which data points far from the main body of data are down-weighted can be used to account for outliers. Nevertheless, when testing for the significance of r, the data should be normally distributed. In rank systems (Section 2.4.2) the ranking mitigates against outliers so data transformation is not needed. Otherwise a log-transformation of the data (Section 2.7) may be warranted.

### Rank Correlation Coefficients

Sometimes geochemical data cannot be used in product moment correlation of the type described above as they do not fulfil the requisite conditions of being normally distributed and excluding outliers. Under such circumstances, an alternative to the Pearson product-moment coefficient of linear correlation is the non-parametric rank correlation coefficient. Both the Spearman and Kendall rank correlation coefficients can be used to determine the strength of the relationship between two variables. In most situations, the interpretations of the Kendall’s tau and Spearman’s rank correlation coefficients are similar and lead to the same inferences, although the Spearman coefficient is more widely used. Both methods can be performed in Excel.

#### Spearman Rank Correlation

The Spearman rank coefficient of correlation is usually designated rs. This type of correlation is applicable to major or trace element data measured on a ranking scale rather than the equidistant scale used in Pearson’s product-moment correlation. The Spearman rank correlation coefficient is defined as:

$$
rs=1–[6 \sum \frac{D^2}{n(n^2−1)}]
$$

where D is the difference in ranking between the x-values and y-values and n is the number of pairs. In this case the only assumptions are that x and y are continuous random variables, which are at least ranked and are independent paired observations. If the rank orders are the same, then D = 0 and rs = +1.0. If the rank orders are the reverse of each other, rs = −1.0. The significance of rs can be assessed using significance tables for the Spearman rank coefficient of correlation (Table 2.3b) in a similar way to that described for product-moment correlation in Section 2.4.1. The Spearman rank coefficients of correlation for the major element data from the Limpopo Belt are shown in Table 2.3b. In this instance, the values do not differ greatly from the Pearson product moment coefficient of correlation. The particular advantages of the Spearman rank correlation coefficient are that they are applicable to ranked data and are superior to the product-moment correlation coefficient when applied to populations that are not normally distributed and/or include outliers. A further advantage is that the Spearman rank correlation coefficient (rS) is quickly and easily calculated and can be used as an approximation for the product-moment correlation coefficient (r).

#### Kendall Rank Correlation

The Kendall rank correlation coefficient (Greek letter τ, tau) is another measure of rank correlation that is used to assess the similarity of the orderings of data when ranked by two respective variables. When +1, the agreement between the two rankings is perfect and the two rankings are the same; if 0, the rankings are completely independent; if −1, the disagreement between the two rankings is perfect and one ranking is the reverse of the other. The general formula for Kendall’s correlation is:

$$
τ= \frac{n_c−n_d}{n(n−1)/2}
$$

where $n_c$ is the number of concordant pairs, $n_d$ is the number of discordant pairs in a ranking of two variables, and n is the number of ranked variables. Kendall’s correlation coefficient is generally smaller than Spearman’s, is less sensitive to the size of the deviation between the rank ordered pairs than Spearman’s, has better statistical properties (the sample estimate is close to the population variance so confidence levels are more reliable) and is better for small sample sizes (≲12). Significance levels associated with the Kendall rank coefficient of correlation for small datasets are calculated individually. Larger datasets tend to converge on a normal distribution and significance can be determined using the z-value:

$$
z= \frac{3 \times τ \sqrt{n(n−1)}}{\sqrt{2(2n+5)}}
$$

where n is the number of ranked variables. The critical significance table for Kendall’s tau is given in Table 2.5.

### The Strength of a Correlation

The correlation coefficient, r, estimates the strength and direction of a linear relationship between x and y. The reliability of the linear model, however, is also a function of the number of observed data points in the population such that the sample size (n) and r must be considered together. As shown in Section 2.4.1.1, when the calculated significance is greater than the critical value for the number of samples in the dataset (for bivariate data, n − 2), the correlation coefficient of the dataset is statistically significant. There is no single cut-off that defines a ‘strong’ linear correlation but values of r ≥ 0.7 and τ ≥ 0.5 are generally taken to indicate good to strong correlations. In geochemistry the significance of a correlation is usually assigned at the 5% level (or 95% confidence level). The coefficient of determination ( $R^2$ ) is the square of r. The usefulness of $R^2$ is that there is no distinction between positive and negative values (as with r) and it ranges in value from 0 to 1.0. It is therefore important to specify the significance or confidence level and n for all values of r or $R^2$ reported.

### Correlation and Non-homogeneous Data

Some statistical tests require or assume a homogeneous sample distribution and applying such a test to a non-homogeneous dataset can yield false correlations. For example, consider a dataset of nine basaltic andesites and nine andesites (Figure 2.2). When combined in a bivariate plot the two compositional groups are strongly correlated (r = 0.922), well above the 5% significance limit in Table 2.3a. However, when assessed independently the basaltic andesites are not significantly correlated with each other (r=0.458, below the 5% significance limit in Table 2.3a). In this case, the apparent correlation between the basaltic andesites and the andesites may reflect the incorrect combination of two distinct sub-populations into a single non-homogeneous group. Understanding the geological field relationships is critical therefore to identifying the correct statistical treatment of data.

![False correlations in non-homogeneous data.](pic/fig2-2.png "非同质数据中的错误相关性")

_Nine basaltic andesites (squares) and nine andesites (circles) yield a strong linear correlation (r = 0.922 at >5% significance), whereas the basaltic andesites in the subgroup (r = 0.458) are not correlated at the 5% significance level._

Another consideration concerns outliers (Section 2.2.2.4). For normally distributed data, outliers can be defined by the ‘3σ rule’: the deviation from the mean by more than three times the standard deviation. This means that only one in 370 samples should deviate from the mean. Deletion of outlier data is controversial especially in small datasets or those in which a normal distribution cannot be assumed. Rejection of outliers is more acceptable when associated with instrument measurements and the distribution of measurement error is known. In the case of measurement error, outliers should either be discarded or a form of statistics used that is robust to outliers such as the ranked coefficient of correlation. In any case the exclusion of any data point(s) should be explicitly reported. Where a distribution is skewed, a non-Gaussian distribution is implied, and statistical tests suited for non-normal distributions should be applied.

### Correlation Matrices

Frequently, a geochemical dataset will have as many as 30 variables. This means that there are 435 possible bivariate diagrams (Section 3.3.2) for a single dataset. While simple bivariate plots provide information about the data, its structure and interrelationships, making more than 400 plots is not an efficient way to proceed. As an initial step, the creation of a correlation matrix allows significant correlations to be quickly and easily identified. This may be a prelude to making selected bivariate plots or to applying more sophisticated statistical analyses. A correlation matrix requires determining a correlation coefficient between variable pairs and presenting the results as a matrix as in Table 2.4. Rahimi et al. (2016) used a correlation matrix to identify correlated rare earth elements in their investigation of the Lakehsiya ore deposit and from the observed correlations were able to identify those accessory minerals which were the host to the REE identified. It should be noted, however, that (1) extreme outliers can perturb a correlation matrix; therefore, decisions regarding the handling of outliers are required before creating the matrix, and that (2) although the correlation matrix is useful and correlation coefficients are convenient statistical descriptors, caution is needed in their application to geochemical data because of their unusual ‘closed’ nature (see Section 2.2).

## Regression Analysis

Often in geochemistry the strength of an association, as defined by the correlation coefficient, is sufficient information from which petrological conclusions may be drawn. Sometimes, however, it is useful to quantify that association. This is traditionally done using regression analysis. For example, regarding the association between CaO and Al2O3 in the tonalites and trondhjemites of Table 2.2 the question ‘If the CaO concentration were 3.5 wt.%, what would be the concentration of Al2O3?’ can be answered with a linear regression of the two variables CaO and Al2O3. The quantification of this association is carried out by fitting a straight line through the data and finding the equation of its line. The equation for a straight line relating variables x and y is:

$$ y=a+bx $$

The constant a is the value of y given by the straight line at x = 0. The constant b is the slope of the line and shows the number of units in y (increase or decrease) that accompanies an increase in one unit of x. The constants a and b are determined by fitting the straight line to the data. The relation above is ideal and does not allow for any deviation from the line. In reality this is not the case, for most observations have associated error(s) and the data may form a cloud of points to which a straight line must be fitted. This introduces some uncertainty into line-fitting procedures resulting in a number of alternative approaches. Regression analysis is the subject of a number of statistical texts; see, for example, Bingham and Fry (2010) and Montgomery et al. (2012). Below some of the more common forms of regression are described.

### Ordinary Least Squares (OLS) Regression

Ordinary least squares (OLS) regression (also known as simple regression) is one of the most commonly used line-fitting techniques in geochemistry because it is simple to use and included in most spreadsheet software. It requires that the units for both variables are the same and are normally distributed. However, it is often inappropriate for geochemical data because there is the further assumption that one variable is independent and error-free. The least squares best-fit line is constructed so that the sum of the squares of the vertical deviations about the line is a minimum. In this case the variable x is the independent (non-random) variable and is assumed to be without error; y, on the other hand, is the dependent (random) variable with associated errors. In this case we say that y is regressed on x (Figure 2.3a). It is possible to regress x on y and in this case the best fit line minimises the sum of the squares of the horizontal deviations about the line (Figure 2.3b). Thus, there are two possible regression lines for the same data, a rather unsatisfactory situation for physical scientists who prefer a unique solution. The two lines intersect at the mean of the sample (Figure 2.3c) and approach each other as the value of the correlation coefficient (r) increases until they coincide at r = 1. In the case of ordinary least squares regression (y is regressed on x), the value of the intercept, a, may be computed from:

$$ a=y−bx $$

where x and y are the mean values for variables x and y, and b is the slope of the line computed from

$$ b=r(Sy/Sx) $$

where r is the Pearson product-moment correlation coefficient and Sx and Sy are the standard deviations of the samples of x and y values. Linear confidence intervals can be expressed in terms of standard deviations (Montgomery et al., 2012). Thus, confidence intervals on values of y for a number of values of x may be used to draw a confidence band on the regression line. The confidence band will be wider at the ends of the fitted line because there are more points near the mean values.

![](pic/fig2-3.png)

 Types of linear regression. (a) Ordinary least squares regression of y on x; in this case the vertical distance between the point and the line is minimised. (b) Ordinary least squares regression of x on y; the horizontal distance between the point and the line is minimised. (c) Both ordinary least squares lines pass through the means (x̄, ȳ), the centroid of the data. (d) Orthogonal regression which minimises the orthogonal distance from the observed data points to the regression line. (e) Reduced major axis regression; the line is fitted to minimise the area of the shaded triangles.